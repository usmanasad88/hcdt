<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HCDT Project - GitHub Pages</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .media-section {
            margin: 20px 0;
            padding: 15px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
        }
        video {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>
<body>
    <header>
        <h1>Project Page</h1>
        <p>Proactive Robotic Assistance via Context-Aware Human Intent and Motion Prediction in Human-Centric Digital Twins</p>
    </header>

    <main>
        <section>
            <h2>About This Research</h2>
            <p>This repository contains research on developing proactive robotic assistants capable of anticipating human needs and actions in complex, semi-structured environments. The project focuses on Human-Centric Digital Twins (HCDTs) that can predict human intent and generate corresponding 3D motion for enhanced human-robot collaboration.</p>
            
            <h3>Key Research Areas:</h3>
            <ul>
                <li><strong>Intent Prediction:</strong> Using AI foundation models to understand human intentions from multimodal cues</li>
                <li><strong>Motion Generation:</strong> Creating physically plausible 3D human motion sequences based on predicted intent</li>
                <li><strong>Context Awareness:</strong> Leveraging scene understanding and task knowledge for better predictions</li>
                <li><strong>Modular Framework:</strong> Integrating pre-trained models for perception, reasoning, and motion synthesis</li>
            </ul>

            <h3>Technical Approach:</h3>
            <p>Our modular framework combines state-of-the-art components including:</p>
            <ul>
                <li>Vision-Language Models (VLMs) for scene understanding</li>
                <li>Large Language Models (LLMs) for high-level reasoning about tasks and intent</li>
                <li>Diffusion models for physics-aware human motion generation</li>
                <li>3D pose estimation using SMPL-X representation</li>
                <li>Gaze tracking and attention analysis for intent cues</li>
            </ul>

            <p>The system is designed to be particularly beneficial for Small and Medium Enterprises (SMEs) seeking adaptable human-robot collaboration solutions without requiring extensive end-to-end model retraining.</p>
        </section>

        <section class="media-section">
            <h2>Project Results</h2>
            <p>Visualization of our prediction results and framework performance:</p>
            <img src="result.png" alt="Visualization of the Prediction Results">
        </section>

        <section class="media-section">
            <h2>Demo Videos</h2>
            <p>Watch our project demonstration videos:</p>
            <video controls>
                <source src="result.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </section>
    </main>

    <footer>
        <p>Â© 2024 HCDT Project - Capital University of Science and Technology & Collaborators</p>
    </footer>
</body>
</html>
