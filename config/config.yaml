defaults:
  - exp: Cooking
  - _self_

case_study: Cooking
use_ground_truth: false
total_frames_to_process: all
use_file_upload: false
use_openai: false

start_frame: 8940
end_frame: 0
max_frames: 8000
phase_two_increment: 120
prediction_delay_seconds: 2

model: gemini-2.5-flash-lite-preview-06-17 #gemini-2.5-flash #gemini-2.0-flash, gemini-2.5-flash-lite-preview-06-17 , gemini-2.5-flash gemma-3-27b-it gemma-3-27b-it

use_gaze: true
gaze_prompt: Images of heatmaps of the gaze target of the subject performing the task are also provided.

prompts:
  version1: |
    You are an AI assistant analyzing video frames of a person performing a task.
    Your goal is to update the state variables based on the provided task graph, state schema, and the visual information from a series of images.

    Task Graph Definition:
    ```json
    {task_graph_string}
    ```

    State Variables Schema:
    ```json
    {state_schema_string}
    ```

    Instruction:
    
    First, as an example, you are being provided with a series of images {example_frame_step} / {fps} = {example_frame_step_seconds:.2f} seconds apart for the same task performed by a different subject.
    Next, you will be provided with frames {test_frame_step} / {fps} = {test_frame_step_seconds:.2f} seconds apart for the task performed by the test subject.
    
    The ground truth states for the example is provided below:
    ```json
    {example_gt_string}
    ```
    
  version2: |
    You are an AI assistant analyzing video frames of a person performing a task.
    Your goal is to update the state variables based on the provided task graph, state schema, and the visual information from a series of images.

    Instruction:
    
    First, as an example, you are being provided with a series of images {example_frame_step_seconds:.2f} seconds apart for the same task performed by a different subject.
    Next, you will be provided with frames {test_frame_step_seconds:.2f} seconds apart for the task performed by the test subject. Based on the image provided and the schemas above, update the state variables.
    Boolean state variables are not strictly boolean and can be False, True or Unknown.
    Output the updated state variables as a JSON object.
    The ground truth states for the example is also provided. Analyse provided the information.    
    ```
  version3: |
    You are an AI assistant analyzing video frames of a person performing a task.
    Your goal is to update the state variables based on the provided task graph, state schema, and the visual information from a series of images.

    Task Graph Definition:
    ```json
    {task_graph_string}
    ```

    State Variables Schema:
    ```json
    {state_schema_string}
    ```

    Instruction:
    
    First, as an example, you are being provided with a series of images {example_frame_step} / {fps} = {example_frame_step_seconds:.2f} seconds apart for the same task performed by a different subject.
    Next, you will be provided with frames {test_frame_step} / {fps} = {test_frame_step_seconds:.2f} seconds apart for the task performed by the test subject.
    
    The example video frames are provided below:
    ```
  getgroundtruth: |
    You are an AI assistant analyzing video frames of a person performing a task.
    Your goal is to update the state variables based on the provided task graph, state schema, and the visual information from a series of images.

    Instruction:
    
    You are being provided with a series of images 1 seconds apart 
    Based on the images provided and the schemas above, update the state variables.
    Boolean state variables are not strictly boolean and can be False, True or Unknown.
    Output the updated state variables as a JSON object.    .

    
    ```
  #  Each input image will be 1280Ã—720 pixels in size. 

  phase_two: |
    You are an AI assistant analyzing video frames of a person performing a task.
    Currently, the person is performing this subtask: {task_description_string}
   
    You will be provided with a sequence of 10 images, each captured 0.2 seconds apart. 
    For each image, you will also receive an overlay indicating the human pose and the precise pixel locations of both the left and right hands. 
    The pixel locations (x, y coordinates) will be normalized to a range of 0 to 1000.
    The velocities of the left and right hands in cm/s will also be provided.
    Your task is to analyze this sequence of images and the associated pose and hand data. 
    Based on this input, predict the exact pixel locations for both the left hand and the right hand two seconds after the timestamp of the very last image in the provided sequence.
    Do not simply extrapolate the data, estimate where the hands will be at that time based on the task being performed.
    Provide your prediction as a structured output, clearly indicating the predicted (x, y) coordinates for each hand, with values normalized from 0 to 1000, as well as the name of the target object being manipulated in the task. 
    Also provide a one line reasoning summary of your prediction. 

second_prompt: 
  v1: |
    Now the history of frames for the test video is provided.
    Ensure your output is only the JSON object representing the updated state variables, for the last provided frame (frame number: {frame_num}). 
    History of frames uptil {last_frame_time} second(s) at frame number {frame_num} at {fps} fps :
    ```

